{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link : https://www.kaggle.com/datasets/vasukipatel/face-recognition-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new dataset by cropping only faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/how-to-crop-and-save-the-detected-faces-in-opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(img_path):\n",
    "    #take imagepath, crops the face, and returns it\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)        #convert image to grayscale\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')        #read haarcascade file\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "\n",
    "    # loop over all detected faces\n",
    "    if(len(faces) > 0):\n",
    "        #we are assuming that only one face in each image\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        return face\n",
    "\n",
    "    return None         #if no face is found simply return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glob to go through all the images\n",
    "#read image and crop faces using opencv and haar-cascade\n",
    "#save inside cropped > akshay Kumar > new image\n",
    "\n",
    "\n",
    "for img_path in glob.glob(\"data/celeb_images/*/*.jpg\"):\n",
    "    temp = crop_face(img_path)\n",
    "\n",
    "    if(temp is not None):\n",
    "        folder_name = img_path.split('/')[-2]\n",
    "        #chck if this folder exists\n",
    "        #if yes, save in it, if no, create then save in it\n",
    "\n",
    "        if(os.path.exists(\"data/celeb_images_cropped/\" + folder_name)):\n",
    "            cv2.imwrite(\"data/celeb_images_cropped/\" + folder_name + \"/\" + str(uuid.uuid4()) + \".jpg\", temp)\n",
    "        else:\n",
    "            os.mkdir(\"data/celeb_images_cropped/\" + folder_name)\n",
    "            cv2.imwrite(\"data/celeb_images_cropped/\" + folder_name + \"/\" + str(uuid.uuid4()) + \".jpg\", temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffling and creating test, train, val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of dicts {'image' : imagepath, 'label' : celeb_name}\n",
    "#then shuffle them\n",
    "#then partition and convert to dataframes\n",
    "\n",
    "parent = []\n",
    "\n",
    "for img_path in glob.glob(\"data/celeb_images_cropped/*/*.jpg\"):\n",
    "    temp = img_path.split(\"/\") \n",
    "    parent.append({'image' : img_path, \"label\" : temp[-2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(parent)      #shuffle the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356 1884 236 236\n"
     ]
    }
   ],
   "source": [
    "#now divide into train, test, val\n",
    "train_list = parent[0 : int(len(parent) * 0.8)]\n",
    "test_list = parent[int(len(parent) * 0.8) : int(len(parent) * 0.9)]\n",
    "val_list = parent[int(len(parent) * 0.9) : ]\n",
    "\n",
    "print(len(parent), len(train_list), len(test_list), len(val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "test_df = pd.DataFrame(test_list)\n",
    "val_df = pd.DataFrame(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.0 in ./.venv/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.25.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (71.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.venv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0\n",
    "\n",
    "#pip install --force-reinstall \"tensorflow==2.15.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 16:20:00.814565: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-19 16:20:00.816321: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-19 16:20:00.840917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 16:20:00.840962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 16:20:00.841861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 16:20:00.847396: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-19 16:20:00.847750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 16:20:01.399776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ImageDataGenerator(rescale= 1./255)\n",
    "train_gen = ImageDataGenerator(rescale= 1./255)\n",
    "val_gen = ImageDataGenerator(rescale= 1./255)\n",
    "\n",
    "#extract class names\n",
    "class_names = set()\n",
    "for img in glob.glob(\"data/celeb_images/*/*.jpg\"):\n",
    "    temp = img.split(\"/\")\n",
    "    class_names.add(temp[-2])\n",
    "class_names = list(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kashyap',\n",
       " 'Ellen Degeneres',\n",
       " 'Virat Kohli',\n",
       " 'Anushka Sharma',\n",
       " 'Vijay Deverakonda',\n",
       " 'Billie Eilish',\n",
       " 'Claire Holt',\n",
       " 'Dwayne Johnson',\n",
       " 'Alia Bhatt',\n",
       " 'Brad Pitt',\n",
       " 'Marmik',\n",
       " 'Amitabh Bachchan',\n",
       " 'Tom Cruise',\n",
       " 'Elizabeth Olsen',\n",
       " 'Akshay Kumar',\n",
       " 'Margot Robbie',\n",
       " 'Natalie Portman',\n",
       " 'Henry Cavill',\n",
       " 'Hrithik Roshan',\n",
       " 'Priyanka Chopra',\n",
       " 'Robert Downey Jr',\n",
       " 'Camila Cabello',\n",
       " 'Jessica Alba',\n",
       " 'Zac Efron',\n",
       " 'Lisa Kudrow',\n",
       " 'Courtney Cox',\n",
       " 'Alexandra Daddario',\n",
       " 'Charlize Theron',\n",
       " 'Andy Samberg',\n",
       " 'Hugh Jackman',\n",
       " 'Roger Federer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 236 validated image filenames belonging to 31 classes.\n",
      "Found 1884 validated image filenames belonging to 31 classes.\n",
      "Found 236 validated image filenames belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "#create datasets\n",
    "test_dataset = test_gen.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    x_col = 'image',\n",
    "    y_col = 'label',\n",
    "    target_size = (224, 224),\n",
    "    classes = class_names,\n",
    "    class_mode = \"sparse\",\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "train_dataset = train_gen.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    x_col = 'image',\n",
    "    y_col = 'label',\n",
    "    target_size = (224, 224),\n",
    "    classes = class_names,\n",
    "    class_mode = \"sparse\",\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val_dataset = val_gen.flow_from_dataframe(\n",
    "    dataframe = val_df,\n",
    "    x_col = 'image',\n",
    "    y_col = 'label',\n",
    "    target_size = (224, 224),\n",
    "    classes = class_names,\n",
    "    class_mode = \"sparse\",\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = tf.keras.applications.MobileNetV2(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = (224, 224, 3)\n",
    ")\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    conv_base,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(31, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = 'adam',\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (10.4.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (1.14.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in ./.venv/lib/python3.10/site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "59/59 [==============================] - 36s 578ms/step - loss: 2.9059 - accuracy: 0.1906 - val_loss: 2.3661 - val_accuracy: 0.3305\n",
      "Epoch 2/15\n",
      "59/59 [==============================] - 34s 574ms/step - loss: 1.8500 - accuracy: 0.4772 - val_loss: 1.7574 - val_accuracy: 0.4873\n",
      "Epoch 3/15\n",
      "59/59 [==============================] - 34s 569ms/step - loss: 1.3250 - accuracy: 0.6285 - val_loss: 1.5382 - val_accuracy: 0.5424\n",
      "Epoch 4/15\n",
      "59/59 [==============================] - 33s 557ms/step - loss: 1.0198 - accuracy: 0.7155 - val_loss: 1.3827 - val_accuracy: 0.6059\n",
      "Epoch 5/15\n",
      "59/59 [==============================] - 34s 567ms/step - loss: 0.8047 - accuracy: 0.7972 - val_loss: 1.2231 - val_accuracy: 0.6398\n",
      "Epoch 6/15\n",
      "59/59 [==============================] - 34s 571ms/step - loss: 0.6393 - accuracy: 0.8461 - val_loss: 1.1779 - val_accuracy: 0.6483\n",
      "Epoch 7/15\n",
      "59/59 [==============================] - 32s 549ms/step - loss: 0.5093 - accuracy: 0.8896 - val_loss: 1.1618 - val_accuracy: 0.6568\n",
      "Epoch 8/15\n",
      "59/59 [==============================] - 32s 542ms/step - loss: 0.4210 - accuracy: 0.9156 - val_loss: 1.0813 - val_accuracy: 0.6610\n",
      "Epoch 9/15\n",
      "59/59 [==============================] - 31s 527ms/step - loss: 0.3437 - accuracy: 0.9411 - val_loss: 1.0703 - val_accuracy: 0.6822\n",
      "Epoch 10/15\n",
      "59/59 [==============================] - 31s 519ms/step - loss: 0.2782 - accuracy: 0.9591 - val_loss: 1.0613 - val_accuracy: 0.6780\n",
      "Epoch 11/15\n",
      "59/59 [==============================] - 33s 554ms/step - loss: 0.2252 - accuracy: 0.9735 - val_loss: 1.0133 - val_accuracy: 0.7076\n",
      "Epoch 12/15\n",
      "59/59 [==============================] - 34s 569ms/step - loss: 0.1898 - accuracy: 0.9814 - val_loss: 1.0739 - val_accuracy: 0.6949\n",
      "Epoch 13/15\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 0.1532 - accuracy: 0.9883 - val_loss: 1.0368 - val_accuracy: 0.7076\n",
      "Epoch 14/15\n",
      "59/59 [==============================] - 38s 641ms/step - loss: 0.1290 - accuracy: 0.9920 - val_loss: 1.0178 - val_accuracy: 0.6907\n",
      "Epoch 15/15\n",
      "59/59 [==============================] - 43s 729ms/step - loss: 0.1030 - accuracy: 0.9973 - val_loss: 1.0109 - val_accuracy: 0.7161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1dea163b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 15, \n",
    "    validation_data = (val_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"celeb_face_recog.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 4s - loss: 1.2965 - accuracy: 0.6441 - 4s/epoch - 500ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2965295314788818, 0.6440678238868713]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(\n",
    "    test_dataset, verbose = 2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
